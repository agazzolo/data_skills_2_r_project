provider_sum %>%
filter(State == "IL") %>%
arrange(desc(Avg_Pymt_Amt_Per_Epsd)) %>%
head(5)
provider_sum %>%
filter(State == "IL") %>%
arrange(desc(cost_norm)) %>%
head(5)
library(tidyverse)
library(readr)
library(lintr)
library(styler)
library(testthat)
library(readxl)
library(tidycensus)
library(lubridate)
CENSUS_KEY <- Sys.getenv("CENSUS_API_KEY")
acs_vars_14 <- load_variables(2014, "acs5", cache = TRUE)
census <- get_acs(geography = "zcta", variable = c("B01003_001", "B01001B_001", "B19013_001"), year = 2014, state = "Illinois", survey = "acs5")
knitr::opts_chunk$set(echo = TRUE)
-3.64-3.98
3.64-3.98
knitr::opts_chunk$set(echo = TRUE)
library(bigrquery)
billing_and_proj_id <- "ganong-teaching-343718"
sql <- "SELECT
*
FROM `waze-public-dataset.partner_UniversityofChicago.view_alerts_clustered`
WHERE city = 'Chicago, IL' AND date(ts) = '2021-03-01'"
tbl_waze <- bq_project_query(billing_and_proj_id, sql)
library(tidyverse)
library(lintr)
library(styler)
library(testthat)
library(readxl)
library(tidycensus)
library(nycflights13)
flights <- nycflights13::flights
weather <- nycflights13::weather
View(weather)
library(nycflights13)
flights <- nycflights13::flights
weather <- nycflights13::weather
flights_100 <- flights %>% head(100)
weather_100 <- weather %>% head(100)
system.time(flights_weather <- left_join(flights_100, weather_100, by = "year", suffix = "we"))
flights_100 <- flights %>% head(100)
weather_100 <- weather %>% head(100)
system.time(flights_weather <- left_join(flights_100, weather_100, by = "year", suffix = c("_flights", "_weather"))
system.time(flights_weather <- left_join(flights_100, weather_100, by = "year", suffix = c("_flights", "_weather"))
system.time({flights_weather <- left_join(flights_100, weather_100, by = "year", suffix = c("_flights", "_weather")})
system.time(flights_weather <- left_join(flights_100, weather_100, by = "year", suffix = c("_flights", "_weather")))
View(flights_weather)
100*100
336776 * 26115
system.time(flights_weather <- left_join(flights_100, weather_100, by = "year", suffix = c("_flights", "_weather")))
library(tidyverse)
library(lintr)
library(styler)
library(testthat)
library(readxl)
library(tidycensus)
library(nycflights13)
flights <- nycflights13::flights
weather <- nycflights13::weather
flights_100 <- flights %>% head(100)
weather_100 <- weather %>% head(100)
system.time(flights_weather <- left_join(flights_100, weather_100, by = "year", suffix = c("_flights", "_weather")))
(8794905240/10000)*0.005
((8794905240/10000)*0.005)/60
View(flights)
flights %>%
filter(month == 6 & day == 13) %>%
summarise(avg_dep_delay = mean(dep_delay))
flights %>%
filter(month == 6 & day == 13) %>%
summarise(avg_dep_delay = mean(dep_delay), na.rm = TRUE)
flights %>%
filter(month == 6 & day == 13) %>%
summarise(avg_dep_delay = mean(dep_delay, na.rm = TRUE))
flights %>%
summarise(avg_dep_delay = mean(dep_delay, na.rm = TRUE))
flights %>%
filter(is.na(tailnum))
flights %>% anti_join(airports, by = c("dest" = "faa"))
airports %>% anti_join(flights, by = c("faa" = "dest"))
make_datetime_100 <- function(year, month, day, time) {
make_datetime(year, month, day, time %/% 100, time %% 100)
}
flights_dt <- flights %>%
filter(!is.na(dep_time), !is.na(arr_time)) %>%
mutate(
dep_time = make_datetime_100(year, month, day, dep_time),
arr_time = make_datetime_100(year, month, day, arr_time),
sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
) %>%
select(origin, dest, ends_with("delay"), ends_with("time"))
library(lubridate)
make_datetime_100 <- function(year, month, day, time) {
make_datetime(year, month, day, time %/% 100, time %% 100)
}
flights_dt <- flights %>%
filter(!is.na(dep_time), !is.na(arr_time)) %>%
mutate(
dep_time = make_datetime_100(year, month, day, dep_time),
arr_time = make_datetime_100(year, month, day, arr_time),
sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
) %>%
select(origin, dest, ends_with("delay"), ends_with("time"))
flights_dt %>%
mutate(est_dep_time = sched_dep_time + dep_delay) %>%
filter(est_dep_time != dep_time)
flights_dt %>%
mutate(est_dep_time = sched_dep_time + dep_delay*60) %>%
filter(est_dep_time != dep_time)
flights_dt %>%
mutate(est_dep_delay = dep_time - sched_dep_time) %>%
filter(est_dep_delay != dep_delay)
flights_dt %>%
mutate(est_dep_delay = dep_time - sched_dep_time) %>%
filter(est_dep_delay/60 != dep_delay)
d1 <- "1213-Apr-03"
d2 <- "06-Jun-2017"
d3 <- "12/29/14" # Dec 29, 2014
d4 <- "November 20, 1909"
d5 <- c("January 2 (2016)", "January 2 (2018)")
ymd(d1)
dmy(d2)
mdy(d3)
mdy(d4)
mdy(d5)
flights_dt %>%
mutate(week_day = wday(sched_dep_time)) %>%
group_by(week_day) %>%
summarise(
dep_delay = length(dep_delay > 0)/length(dep_delay),
arr_delay = length(arr_delay > 0)/length(arr_delay))
flights_dt %>%
mutate(week_day = wday(sched_dep_time)) %>%
group_by(week_day) %>%
summarise(
dep_delay = count(dep_delay > 0)/length(dep_delay),
arr_delay = count(arr_delay > 0)/length(arr_delay))
flights_dt %>%
mutate(week_day = wday(sched_dep_time)) %>%
group_by(week_day) %>%
summarise(
dep_delay = length(dep_delay > 0)/length(dep_delay),
arr_delay = length(arr_delay > 0)/length(arr_delay))
ymd("2012-01-07") + months(0:11)
ymd("2012-01-07") + dmonths(0:11)
ymd("2012-01-07") + dmonths(0:11)
ymd("2020-01-05") + dmonths(0:11)
ymd("2012-01-07") + months(0:11)
ymd("2020-01-05") + months(0:11)
f <- function(string, prefix) { str_sub(string, 1, nchar(prefix)) == prefix
}
g <- function(x) {
if (length(x) <= 1) return(NULL) x[-length(x)]
f <- function(string, prefix) { str_sub(string, 1, nchar(prefix)) == prefix
}
g <- function(x) {
if (length(x) <= 1) return(NULL)
x[-length(x)]
}
f(c("apple", "banana", "apply"), "ap")
g(c(1,3,4))
g(c(1,3,4,7))
f <- function(string, prefix) { str_sub(string, 1, nchar(prefix)) == prefix
}
g <- function(x) {
if (length(x) <= 1) return(NULL)
x[-length(x)]
}
f(c("apple", "banana", "apply"), "ap")
g(c(1,3,4,7))
prefix <- f
remove_last <- g
greeting <- function(time = now()) {
hour <- hour(time)
if (hour < 12) {
print("good morning")
} else if (hour < 17) {
print("good afternoon")
} else {
print("good evening")
}
greeting <- function(time = now()) {
hour <- hour(time)
if (hour < 12) {
print("good morning")
} else if (hour < 17) {
print("good afternoon")
} else {
print("good evening")
}
}
greeting()
greeting(ms("08:30"))
greeting(ms("15:00"))
greeting(ms("20:00"))
greeting(ms("08:30"))
greeting(ms("15:00"))
greeting(ms("20:00"))
greeting(ms("15:00"))
greeting <- function(time = now()) {
hr <- hour(time)
if (hr < 12) {
print("good morning")
} else if (hr < 17) {
print("good afternoon")
} else {
print("good evening")
}
}
greeting(ms("08:30"))
greeting(ms("15:00"))
greeting(ms("20:00"))
greeting <- function(time = now()) {
hour <- hour(time)
if (hour < 12) {
print("good morning")
} else if (hour < 17) {
print("good afternoon")
} else {
print("good evening")
}
}
greeting(ms("08:30"))
greeting(ms("15:00"))
greeting(ms("20:00"))
library(tidyverse)
library(fredr)
?fredr_set_key
fredr_key <- fredr_set_key("e6fdbe0f9c4b5c034204b0f9e17a6f89")
fredr_get_key(fredr_key)
fredr_set_key("e6fdbe0f9c4b5c034204b0f9e17a6f89")
fredr_set_key("e6fdbe0f9c4b5c034204b0f9e17a6f89")
setwd("~/Documents/Data & Programming II/data_skills_2_r_project/Data")
library(shiny); runApp('~/Documents/Data & Programming II/data_skills_2_r_project/shiny.R')
runApp('shiny.R')
library(tidyverse)
library(wordcloud2)
library(shiny)
library(plotly)
library(scales)
runApp('shiny.R')
runApp('shiny.R')
runApp('shiny.R')
runApp('shiny.R')
runApp('shiny.R')
runApp('shiny.R')
runApp('shiny.R')
write.csv(data, "~/Documents/Data & Programming II/data_skills_2_r_project/Data/data_clean.csv", row.names = FALSE)
library(tidyverse)
library(lubridate)
library(rvest)
library(tidycensus)
library(tidytext)
library(nametagger)
library(udpipe)
library(stargazer)
adopt <- read_csv("sac_aggregate_dataset_2019_2021.csv")
covid <- read_csv("us-states-new.csv")
covid$date <- as.Date(covid$date)
covid <- covid %>% mutate(
year = as.numeric(format(date, format = "%Y")),
month = as.numeric(format(date, format = "%m")),
day = as.numeric(format(date, format = "%d")))
covid_yr <- covid %>% group_by(state, year) %>% summarise(cases = sum(cases))
url <- "https://en.wikipedia.org/wiki/U.S._state_and_local_government_responses_to_the_COVID-19_pandemic"
request <- read_html(url)
table <- html_table(request, fill = TRUE)
View(table[[3]])
lockdowns <- table[[3]]
colnames(lockdowns) <- c('State_del',
'State',
'Emergency_Declared',
'Stay_At_Home_Ordered',
'Stay_At_Home_Lifted',
'Masks_Required',
'Gatherings',
'Travel_Restrictions',
'Closures_School',
'Closures_Daycare',
'Closures_Restaurants',
'Closures_Retail',
'Sources')
lockdowns <- lockdowns %>% select(!c(State_del,
Gatherings,
Travel_Restrictions,
Closures_School,
Closures_Daycare,
Closures_Restaurants,
Closures_Retail,
Sources))
lockdowns <- lockdowns %>% mutate(SAH = ifelse(Stay_At_Home_Ordered == 'No', 0, 1))
lockdowns = lockdowns[-1,]
lockdowns <- lockdowns %>% separate(Stay_At_Home_Ordered, c("Start_Month", "Start_Day"))
lockdowns <- lockdowns %>% separate(Stay_At_Home_Lifted, c("End_Month", "End_Day"))
lockdowns$Start_Month <- na_if(lockdowns$Start_Month, "No")
lockdowns$End_Month <- na_if(lockdowns$End_Month, "No")
lockdowns$End_Month <- na_if(lockdowns$End_Month, "N")
lockdowns$End_Day <- na_if(lockdowns$End_Day, "a")
lockdowns$End_Day <- na_if(lockdowns$End_Day, "advisory")
lockdowns$year <- 2020
lockdowns$Start_Date <- as.Date(paste(lockdowns$year, lockdowns$Start_Month, lockdowns$Start_Day, sep = "-"),"%Y-%b-%d")
lockdowns$End_Date <- as.Date(paste(lockdowns$year, lockdowns$End_Month, lockdowns$End_Day, sep = "-"),"%Y-%b-%d")
lockdowns$SAH_Duration <- lockdowns$End_Date - lockdowns$Start_Date
lockdown_clean <- lockdowns %>% select(c(State, SAH, Start_Date, End_Date, SAH_Duration))
feline_intake <- c("Intake - Relinquished By Owner Total-Feline",
"Intake - Stray At Large Total-Feline",
"Intake - Transferred In Total-Feline",
"Intake - Owner Intended Euthanasia Total-Feline")
canine_intake <- c("Intake - Relinquished By Owner Total-Canine",
"Intake - Stray At Large Total-Canine",
"Intake - Transferred In Total-Canine",
"Intake - Owner Intended Euthanasia Total-Canine",
"Intakes - Other Intakes Total-Canine")
adopt$feline_intake_total <- adopt$`Intake - Relinquished By Owner Total-Feline` + adopt$`Intake - Stray At Large Total-Feline` + adopt$`Intake - Transferred In Total-Feline` + adopt$`Intake - Owner Intended Euthanasia Total-Feline` + adopt$`Intakes - Other Intakes Total-Feline`
adopt$canine_intake_total <- adopt$`Intake - Relinquished By Owner Total-Canine` + adopt$`Intake - Stray At Large Total-Canine` + adopt$`Intake - Transferred In Total-Canine` + adopt$`Intake - Owner Intended Euthanasia Total-Canine` + adopt$`Intakes - Other Intakes Total-Canine`
adopt$intake_total <- adopt$feline_intake_total + adopt$canine_intake_total
adopt$adoption_total <- adopt$`Live Outcome - Adoption Total-Feline` + adopt$`Live Outcome - Adoption Total-Canine`
adopt_clean <- adopt %>% select(c(State, Year, feline_intake_total, canine_intake_total, intake_total, `Live Outcome - Adoption Total-Feline`, `Live Outcome - Adoption Total-Canine`, adoption_total))
Sys.getenv("CENSUS_API_KEY")
# variables <- load_variables(2020, "pl", cache = FALSE)
state_pop <- get_decennial(geography = "state",
variables = c(population = "P1_001N"),
year = 2020)
state_pop <- state_pop %>% rename(Population = value) %>%
select(c(NAME, Population))
adopt_clean <- adopt_clean %>%
mutate(State = state.name[match(State, state.abb)])
data <- left_join(adopt_clean, covid_yr, by = c("State" = "state", "Year" = "year"))
data <- left_join(data, state_pop, by = c("State" = "NAME"))
data <- left_join(data, lockdown_clean, by = "State")
# dropping US Territories and adding adoption rate and adoptions per capita
data <- data %>% drop_na(State)
data$adoption_rate <- data$adoption_total/data$intake_total
data$adoption_percap <- data$adoption_total/data$Population
write.csv(data, "~/Documents/Data & Programming II/data_skills_2_r_project/Data/data_clean.csv", row.names = FALSE)
#filtered data frames to simplify plotting
data2020 <- data %>% filter(Year == 2020) %>%
mutate(across(SAH_Duration, ~ ifelse(is.na(.), 0, .)))
data_cases <- data %>% filter(Year != 2019) %>%
mutate(across(cases, ~ ifelse(is.na(.), 0, .)))
ggplot(data2020, aes(SAH_Duration, adoption_percap)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Duration of Stay-At-Home Orders and Adoptions Per Capita, 2020") +
xlab("Duration of Stay-At-Home Orders, in days") +
ylab("Pet Adoptions Per Capita")
ggplot(data2020, aes(SAH_Duration, adoption_rate)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Duration of Stay-At-Home Orders and Pet Adoption Rate, 2020") +
xlab("Duration of Stay-At-Home Orders, in days") +
ylab("Pet Adoption Rate")
ggplot(data_cases, aes(cases, adoption_percap)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Number of COVID-19 Cases and Pet Adoptions Per Capita, 2020-2021") +
xlab("Annual Number of COVID-19 Cases") +
ylab("Pet Adoptions Per Capita")
ggplot(data_cases, aes(cases, adoption_rate)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Number of COVID-19 Cases and Pet Adoption Rate, 2020-2021") +
xlab("Annual Number of COVID-19 Cases") +
ylab("Pet Adoption Rate")
#source for geom_map tips: https://ggplot2.tidyverse.org/reference/geom_map.html
state <- tolower(data$State)
map <- map_data("state")
ggplot(data, aes(map_id = state)) +
geom_map(aes(fill = adoption_rate), map = map) +
coord_sf(
crs = 5070, default_crs = 4326,
xlim = c(-125, -70), ylim = c(25, 52)
) +
facet_wrap(~Year) +
labs(title = "Shelter Pet Adoption Rate in the US, 2019-2021")
ggplot(data, aes(map_id = state)) +
geom_map(aes(fill = adoption_percap), map = map) +
coord_sf(
crs = 5070, default_crs = 4326,
xlim = c(-125, -70), ylim = c(25, 52)
) +
facet_wrap(~Year) +
labs(title = "Shelter Pet Adoption Per Capita in the US, 2019-2021")
reg_adopt_cases <- lm(adoption_total ~ cases + Population, data = data_cases)
stargazer(reg_adopt_cases, type = "text")
reg_adoptrate_cases <- lm(adoption_rate ~ cases + Population, data = data_cases)
stargazer(reg_adoptrate_cases, type = "text")
reg_adopt_SAH <- lm(adoption_total ~ SAH_Duration + Population, data = data2020)
stargazer(reg_adopt_SAH, type = "text")
reg_adoptrate_SAH <- lm(adoption_rate ~ SAH_Duration + Population, data = data2020)
stargazer(reg_adoptrate_SAH, type = "text")
library(tidyverse)
library(rvest)
library(tidytext)
library(nametagger)
library(udpipe)
library(wordcloud2)
library(shiny)
library(plotly)
library(scales)
sentiment_nrc <-
get_sentiments("nrc") %>%
rename(nrc = sentiment)
analyze_text <- function(url, css) {
request <- read_html(url)
article <- html_nodes(request, css)
text_list <- html_text(article)
text <- paste(text_list, collapse = "") %>% tibble(text = text_list)
word_tokens_url <-     unnest_tokens(text, word_tokens,  text, token = "words")
no_sw_df <- anti_join(word_tokens_url, stop_words, by = c("word_tokens" = "word"))
no_sw_df_sent <- no_sw_df %>%
left_join(sentiment_nrc, by = c("word_tokens" = "word"))
return(no_sw_df_sent)
}
i2020_aspca <- analyze_text("https://www.aspcapro.org/news/2020/06/10/over-570-groups-complete-7300-adoptions-during-aspca-national-adoption-weekend", ".post__body")
m2020_nyt <- analyze_text("https://www.nytimes.com/2020/05/06/smarter-living/a-guide-for-first-time-pet-owners-during-the-pandemic.html?searchResultPosition=4",".StoryBodyCompanionColumn")
i2021_dvm <- analyze_text("https://www.dvm360.com/view/making-dermatology-cases-more-efficient-in-general-practice", ".mt-3")
i2021_aspca <- analyze_text("https://www.aspcapro.org/resource/new-aspca-survey-vast-majority-dogs-and-cats-acquired-during-pandemic-still-their-homes", ".paragraph--copy")
m2021_sa <- analyze_text("https://www.scientificamerican.com/article/home-alone-the-fate-of-postpandemic-dogs/", "#sa_body > div.opinion-article__wrapper > section.flex-container.container.flex-direction--column-tablet > article")
m2022_wapo <- analyze_text("https://www.washingtonpost.com/business/2022/01/07/covid-dogs-return-to-work/",".font-copy")
# I was not able to scrape this site so I pulled the text instead
#i2022_pfi <- analyze_text("https://www.petfoodindustry.com/articles/11748-pandemic-new-pet-boom-myth-busted-adoptions-lower-2021-22", "/html/body/div[5]/div[2]/div[1]/div[4]/div[1]")
i2022_pfi <- read_file("petfoodindustry2022.txt")
i2022_pfi_word <- tibble(text = i2022_pfi)
i2022_pfi_word <-  unnest_tokens(i2022_pfi_word, word_tokens,  text, token = "words")
i2022_pfi <- anti_join(i2022_pfi_word, stop_words, by = c("word_tokens" = "word"))
i2022_pfi <- i2022_pfi %>% left_join(sentiment_nrc, by = c("word_tokens" = "word"))
# I attempted to write a function and loop to create the word count dfs but failed
#df_count <- function(df, column) {
#  count(df, column, sort = TRUE)
#}
#df_list <- c("m2020_nyt", "i2020_aspca", "i2021_dvm", "i2021_aspca", "m2021_sa", "m2022_wapo", "i2022_pfi")
#for (i in df_list) {
#output <- df_count(i, nrc)
#}
m2020_nyt_nrc <- count(m2020_nyt, nrc, sort = TRUE) %>% drop_na()
i2020_aspca_nrc <- count(i2020_aspca, nrc, sort = TRUE) %>% drop_na()
i2021_dvm_nrc <- count(i2021_dvm, nrc, sort = TRUE) %>% drop_na()
i2021_aspca_nrc <- count(i2021_aspca, nrc, sort = TRUE) %>% drop_na()
m2021_sa_nrc <- count(m2021_sa, nrc, sort = TRUE) %>% drop_na()
m2022_wapo_nrc <- count(m2022_wapo, nrc, sort = TRUE) %>% drop_na()
i2022_pfi_nrc <- count(i2022_pfi, nrc, sort = TRUE) %>% drop_na()
write.csv(m2020_nyt_nrc, "~/Documents/Data & Programming II/data_skills_2_r_project/Data/nyt2020_nrc.csv", row.names = FALSE)
write.csv(i2020_aspca_nrc, "~/Documents/Data & Programming II/data_skills_2_r_project/Data/aspca2020_nrc.csv", row.names = FALSE)
write.csv(i2021_dvm_nrc, "~/Documents/Data & Programming II/data_skills_2_r_project/Data/dvm2021_nrc.csv", row.names = FALSE)
write.csv(i2021_aspca_nrc, "~/Documents/Data & Programming II/data_skills_2_r_project/Data/aspca2021_nrc.csv", row.names = FALSE)
write.csv(m2021_sa_nrc, "~/Documents/Data & Programming II/data_skills_2_r_project/Data/sa2021_nrc.csv", row.names = FALSE)
write.csv(m2022_wapo_nrc, "~/Documents/Data & Programming II/data_skills_2_r_project/Data/wapo2022_nrc.csv", row.names = FALSE)
write.csv(i2022_pfi_nrc, "~/Documents/Data & Programming II/data_skills_2_r_project/Data/pfi2022_nrc.csv", row.names = FALSE)
# at first, I thought I would create one df with all the articles, but I struggled to make this work in Shiny, so I decided to stick to separate files
nrc_wordcloud_data <- m2020_nyt_nrc %>% full_join(i2020_aspca_nrc, by = "nrc", suffix = c("_20nyt", "_20aspca")) %>%
full_join(i2021_dvm_nrc, by = "nrc", suffix = c("", "_21dvm")) %>%
full_join(i2021_aspca_nrc, by = "nrc", suffix = c("", "_21aspca")) %>%
full_join(m2021_sa_nrc, by = "nrc", suffix = c("", "_21sa")) %>%
full_join(m2022_wapo_nrc, by = "nrc", suffix = c("", "_22wapo")) %>%
full_join(i2022_pfi_nrc, by = "nrc", suffix = c("", "_22pfi"))
nrc_wordcloud_data <- nrc_wordcloud_data %>% rename(n_21dvm = n)
#experimenting with wordclouds before taking it to shiny
# sources I used for wordclouds: https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html
# https://statsandr.com/blog/draw-a-word-cloud-with-a-shiny-app/
# https://shiny.rstudio.com/gallery/word-cloud.html
# https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a
wordcloud(words = nrc_wordcloud_data$nrc, freq = nrc_wordcloud_data$n_20nyt,
min.freq = 1,
max.words = 200,
random.order = FALSE,
random.color = FALSE,
rot.per = 0,
colors = brewer.pal(11, "Spectral"))
wordcloud2(nrc_wordcloud_data,
fontFamily = "sans",
color = brewer.pal(11, "Spectral"))
library(wordcloud)
wordcloud(words = nrc_wordcloud_data$nrc, freq = nrc_wordcloud_data$n_20nyt,
min.freq = 1,
max.words = 200,
random.order = FALSE,
random.color = FALSE,
rot.per = 0,
colors = brewer.pal(11, "Spectral"))
wordcloud2(nrc_wordcloud_data,
fontFamily = "sans",
color = brewer.pal(11, "Spectral"))
wordcloud(words = m2020_nyt_nrc$nrc, freq = m2020_nyt_nrc$n,
min.freq = 1,
max.words = 200,
random.order = FALSE,
random.color = FALSE,
rot.per = 0,
colors = brewer.pal(11, "Spectral"))
wordcloud2(m2020_nyt_nrc,
fontFamily = "sans",
color = brewer.pal(11, "Spectral"))
wordcloud(words = m2020_nyt_nrc$nrc, freq = m2020_nyt_nrc$n,
min.freq = 1,
max.words = 200,
random.order = FALSE,
random.color = FALSE,
rot.per = 0,
colors = brewer.pal(11, "Spectral"))
wordcloud2(m2020_nyt_nrc,
fontFamily = "sans",
color = brewer.pal(11, "Spectral"))
runApp()
